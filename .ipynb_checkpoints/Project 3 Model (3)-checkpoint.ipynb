{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a57a3e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import time\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import regex as re\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier, RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.naive_bayes import MultinomialNB # NLP classification\n",
    "from sklearn.metrics import confusion_matrix, plot_confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2b87f984",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://www.reddit.com/hot.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "85e26734",
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {'User-agent':'Eye eye bot 00'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8e6867df",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = requests.get(url,headers=headers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "47786540",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.status_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b72f34ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "the_json = res.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "41af8931",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['data', 'kind']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(the_json.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "624481ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['after', 'before', 'children', 'dist', 'geo_filter', 'modhash']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(the_json['data'].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e35ac2a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(the_json['data']['children'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e236fb20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>kind</th>\n",
       "      <th>data</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>t3</td>\n",
       "      <td>{'approved_at_utc': None, 'subreddit': 'worldn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>t3</td>\n",
       "      <td>{'approved_at_utc': None, 'subreddit': 'Damnth...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>t3</td>\n",
       "      <td>{'approved_at_utc': None, 'subreddit': 'intere...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>t3</td>\n",
       "      <td>{'approved_at_utc': None, 'subreddit': 'AskRed...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>t3</td>\n",
       "      <td>{'approved_at_utc': None, 'subreddit': 'mildly...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>t3</td>\n",
       "      <td>{'approved_at_utc': None, 'subreddit': 'MadeMe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>t3</td>\n",
       "      <td>{'approved_at_utc': None, 'subreddit': 'HolUp'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>t3</td>\n",
       "      <td>{'approved_at_utc': None, 'subreddit': 'worldn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>t3</td>\n",
       "      <td>{'approved_at_utc': None, 'subreddit': 'KidsAr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>t3</td>\n",
       "      <td>{'approved_at_utc': None, 'subreddit': 'WhiteP...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>t3</td>\n",
       "      <td>{'approved_at_utc': None, 'subreddit': 'rarein...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>t3</td>\n",
       "      <td>{'approved_at_utc': None, 'subreddit': 'Leopar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>t3</td>\n",
       "      <td>{'approved_at_utc': None, 'subreddit': 'politi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>t3</td>\n",
       "      <td>{'approved_at_utc': None, 'subreddit': 'funny'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>t3</td>\n",
       "      <td>{'approved_at_utc': None, 'subreddit': 'todayi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>t3</td>\n",
       "      <td>{'approved_at_utc': None, 'subreddit': 'news',...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>t3</td>\n",
       "      <td>{'approved_at_utc': None, 'subreddit': 'Public...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>t3</td>\n",
       "      <td>{'approved_at_utc': None, 'subreddit': 'OnePie...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>t3</td>\n",
       "      <td>{'approved_at_utc': None, 'subreddit': 'Crappy...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>t3</td>\n",
       "      <td>{'approved_at_utc': None, 'subreddit': 'Funnym...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>t3</td>\n",
       "      <td>{'approved_at_utc': None, 'subreddit': 'league...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>t3</td>\n",
       "      <td>{'approved_at_utc': None, 'subreddit': 'comics...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>t3</td>\n",
       "      <td>{'approved_at_utc': None, 'subreddit': 'Idiots...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>t3</td>\n",
       "      <td>{'approved_at_utc': None, 'subreddit': 'facepa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>t3</td>\n",
       "      <td>{'approved_at_utc': None, 'subreddit': 'datais...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   kind                                               data\n",
       "0    t3  {'approved_at_utc': None, 'subreddit': 'worldn...\n",
       "1    t3  {'approved_at_utc': None, 'subreddit': 'Damnth...\n",
       "2    t3  {'approved_at_utc': None, 'subreddit': 'intere...\n",
       "3    t3  {'approved_at_utc': None, 'subreddit': 'AskRed...\n",
       "4    t3  {'approved_at_utc': None, 'subreddit': 'mildly...\n",
       "5    t3  {'approved_at_utc': None, 'subreddit': 'MadeMe...\n",
       "6    t3  {'approved_at_utc': None, 'subreddit': 'HolUp'...\n",
       "7    t3  {'approved_at_utc': None, 'subreddit': 'worldn...\n",
       "8    t3  {'approved_at_utc': None, 'subreddit': 'KidsAr...\n",
       "9    t3  {'approved_at_utc': None, 'subreddit': 'WhiteP...\n",
       "10   t3  {'approved_at_utc': None, 'subreddit': 'rarein...\n",
       "11   t3  {'approved_at_utc': None, 'subreddit': 'Leopar...\n",
       "12   t3  {'approved_at_utc': None, 'subreddit': 'politi...\n",
       "13   t3  {'approved_at_utc': None, 'subreddit': 'funny'...\n",
       "14   t3  {'approved_at_utc': None, 'subreddit': 'todayi...\n",
       "15   t3  {'approved_at_utc': None, 'subreddit': 'news',...\n",
       "16   t3  {'approved_at_utc': None, 'subreddit': 'Public...\n",
       "17   t3  {'approved_at_utc': None, 'subreddit': 'OnePie...\n",
       "18   t3  {'approved_at_utc': None, 'subreddit': 'Crappy...\n",
       "19   t3  {'approved_at_utc': None, 'subreddit': 'Funnym...\n",
       "20   t3  {'approved_at_utc': None, 'subreddit': 'league...\n",
       "21   t3  {'approved_at_utc': None, 'subreddit': 'comics...\n",
       "22   t3  {'approved_at_utc': None, 'subreddit': 'Idiots...\n",
       "23   t3  {'approved_at_utc': None, 'subreddit': 'facepa...\n",
       "24   t3  {'approved_at_utc': None, 'subreddit': 'datais..."
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(the_json['data']['children'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "54717a4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'approved_at_utc': None,\n",
       " 'subreddit': 'worldnews',\n",
       " 'selftext': '',\n",
       " 'author_fullname': 't2_nexx2v99',\n",
       " 'saved': False,\n",
       " 'mod_reason_title': None,\n",
       " 'gilded': 0,\n",
       " 'clicked': False,\n",
       " 'title': \"Russia blew up about a third of Ukraine's energy infrastructure in two days\",\n",
       " 'link_flair_richtext': [{'e': 'text', 't': 'Russia/Ukraine'}],\n",
       " 'subreddit_name_prefixed': 'r/worldnews',\n",
       " 'hidden': False,\n",
       " 'pwls': 6,\n",
       " 'link_flair_css_class': 'russia',\n",
       " 'downs': 0,\n",
       " 'thumbnail_height': 78,\n",
       " 'top_awarded_type': None,\n",
       " 'hide_score': False,\n",
       " 'name': 't3_y1ppwm',\n",
       " 'quarantine': False,\n",
       " 'link_flair_text_color': 'dark',\n",
       " 'upvote_ratio': 0.95,\n",
       " 'author_flair_background_color': None,\n",
       " 'subreddit_type': 'public',\n",
       " 'ups': 22011,\n",
       " 'total_awards_received': 2,\n",
       " 'media_embed': {},\n",
       " 'thumbnail_width': 140,\n",
       " 'author_flair_template_id': None,\n",
       " 'is_original_content': False,\n",
       " 'user_reports': [],\n",
       " 'secure_media': None,\n",
       " 'is_reddit_media_domain': False,\n",
       " 'is_meta': False,\n",
       " 'category': None,\n",
       " 'secure_media_embed': {},\n",
       " 'link_flair_text': 'Russia/Ukraine',\n",
       " 'can_mod_post': False,\n",
       " 'score': 22011,\n",
       " 'approved_by': None,\n",
       " 'is_created_from_ads_ui': False,\n",
       " 'author_premium': True,\n",
       " 'thumbnail': 'default',\n",
       " 'edited': False,\n",
       " 'author_flair_css_class': None,\n",
       " 'author_flair_richtext': [],\n",
       " 'gildings': {'gid_1': 1},\n",
       " 'post_hint': 'link',\n",
       " 'content_categories': None,\n",
       " 'is_self': False,\n",
       " 'mod_note': None,\n",
       " 'created': 1665536237.0,\n",
       " 'link_flair_type': 'richtext',\n",
       " 'wls': 6,\n",
       " 'removed_by_category': None,\n",
       " 'banned_by': None,\n",
       " 'author_flair_type': 'text',\n",
       " 'domain': 'news.yahoo.com',\n",
       " 'allow_live_comments': True,\n",
       " 'selftext_html': None,\n",
       " 'likes': None,\n",
       " 'suggested_sort': None,\n",
       " 'banned_at_utc': None,\n",
       " 'url_overridden_by_dest': 'https://news.yahoo.com/russia-blew-third-ukraines-energy-193559881.html',\n",
       " 'view_count': None,\n",
       " 'archived': False,\n",
       " 'no_follow': False,\n",
       " 'is_crosspostable': False,\n",
       " 'pinned': False,\n",
       " 'over_18': False,\n",
       " 'preview': {'images': [{'source': {'url': 'https://external-preview.redd.it/FjpjsGs2qr367OI-RbLJMoADwLKwu4tjZnL9c8MPHeE.jpg?auto=webp&amp;s=f814ae272ee37a79bb35bd5fcf89959738df6835',\n",
       "     'width': 690,\n",
       "     'height': 387},\n",
       "    'resolutions': [{'url': 'https://external-preview.redd.it/FjpjsGs2qr367OI-RbLJMoADwLKwu4tjZnL9c8MPHeE.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=065af0a5265fd058aa3868b7edeb62c8e20dfb5d',\n",
       "      'width': 108,\n",
       "      'height': 60},\n",
       "     {'url': 'https://external-preview.redd.it/FjpjsGs2qr367OI-RbLJMoADwLKwu4tjZnL9c8MPHeE.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=14ec58ec005dba91738d7436af765436d0a1bd74',\n",
       "      'width': 216,\n",
       "      'height': 121},\n",
       "     {'url': 'https://external-preview.redd.it/FjpjsGs2qr367OI-RbLJMoADwLKwu4tjZnL9c8MPHeE.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=d712e4198618d21bc99f382e3c52592c5bf61a5e',\n",
       "      'width': 320,\n",
       "      'height': 179},\n",
       "     {'url': 'https://external-preview.redd.it/FjpjsGs2qr367OI-RbLJMoADwLKwu4tjZnL9c8MPHeE.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=ce8026a8323d5084859274d53adc78b3d924c987',\n",
       "      'width': 640,\n",
       "      'height': 358}],\n",
       "    'variants': {},\n",
       "    'id': '1UpmEBG9-gAcRQdh4r471ae60zAmMcKUMtUw64awtd8'}],\n",
       "  'enabled': False},\n",
       " 'all_awardings': [{'giver_coin_reward': None,\n",
       "   'subreddit_id': None,\n",
       "   'is_new': False,\n",
       "   'days_of_drip_extension': None,\n",
       "   'coin_price': 150,\n",
       "   'id': 'award_f44611f1-b89e-46dc-97fe-892280b13b82',\n",
       "   'penny_donate': None,\n",
       "   'award_sub_type': 'GLOBAL',\n",
       "   'coin_reward': 0,\n",
       "   'icon_url': 'https://i.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png',\n",
       "   'days_of_premium': None,\n",
       "   'tiers_by_required_awardings': None,\n",
       "   'resized_icons': [{'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=16&amp;height=16&amp;auto=webp&amp;s=a5662dfbdb402bf67866c050aa76c31c147c2f45',\n",
       "     'width': 16,\n",
       "     'height': 16},\n",
       "    {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=32&amp;height=32&amp;auto=webp&amp;s=a6882eb3f380e8e88009789f4d0072e17b8c59f1',\n",
       "     'width': 32,\n",
       "     'height': 32},\n",
       "    {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=48&amp;height=48&amp;auto=webp&amp;s=e50064b090879e8a0b55e433f6ee61d5cb5fbe1d',\n",
       "     'width': 48,\n",
       "     'height': 48},\n",
       "    {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=64&amp;height=64&amp;auto=webp&amp;s=8e5bb2e76683cb6b161830bcdd9642049d6adc11',\n",
       "     'width': 64,\n",
       "     'height': 64},\n",
       "    {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=128&amp;height=128&amp;auto=webp&amp;s=eda4a9246f95f42ee6940cc0ec65306fd20de878',\n",
       "     'width': 128,\n",
       "     'height': 128}],\n",
       "   'icon_width': 2048,\n",
       "   'static_icon_width': 2048,\n",
       "   'start_date': None,\n",
       "   'is_enabled': True,\n",
       "   'awardings_required_to_grant_benefits': None,\n",
       "   'description': 'Thank you stranger. Shows the award.',\n",
       "   'end_date': None,\n",
       "   'sticky_duration_seconds': None,\n",
       "   'subreddit_coin_reward': 0,\n",
       "   'count': 1,\n",
       "   'static_icon_height': 2048,\n",
       "   'name': 'Helpful',\n",
       "   'resized_static_icons': [{'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=16&amp;height=16&amp;auto=webp&amp;s=a5662dfbdb402bf67866c050aa76c31c147c2f45',\n",
       "     'width': 16,\n",
       "     'height': 16},\n",
       "    {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=32&amp;height=32&amp;auto=webp&amp;s=a6882eb3f380e8e88009789f4d0072e17b8c59f1',\n",
       "     'width': 32,\n",
       "     'height': 32},\n",
       "    {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=48&amp;height=48&amp;auto=webp&amp;s=e50064b090879e8a0b55e433f6ee61d5cb5fbe1d',\n",
       "     'width': 48,\n",
       "     'height': 48},\n",
       "    {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=64&amp;height=64&amp;auto=webp&amp;s=8e5bb2e76683cb6b161830bcdd9642049d6adc11',\n",
       "     'width': 64,\n",
       "     'height': 64},\n",
       "    {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=128&amp;height=128&amp;auto=webp&amp;s=eda4a9246f95f42ee6940cc0ec65306fd20de878',\n",
       "     'width': 128,\n",
       "     'height': 128}],\n",
       "   'icon_format': None,\n",
       "   'icon_height': 2048,\n",
       "   'penny_price': None,\n",
       "   'award_type': 'global',\n",
       "   'static_icon_url': 'https://i.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png'},\n",
       "  {'giver_coin_reward': None,\n",
       "   'subreddit_id': None,\n",
       "   'is_new': False,\n",
       "   'days_of_drip_extension': None,\n",
       "   'coin_price': 100,\n",
       "   'id': 'gid_1',\n",
       "   'penny_donate': None,\n",
       "   'award_sub_type': 'GLOBAL',\n",
       "   'coin_reward': 0,\n",
       "   'icon_url': 'https://www.redditstatic.com/gold/awards/icon/silver_512.png',\n",
       "   'days_of_premium': None,\n",
       "   'tiers_by_required_awardings': None,\n",
       "   'resized_icons': [{'url': 'https://www.redditstatic.com/gold/awards/icon/silver_16.png',\n",
       "     'width': 16,\n",
       "     'height': 16},\n",
       "    {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_32.png',\n",
       "     'width': 32,\n",
       "     'height': 32},\n",
       "    {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_48.png',\n",
       "     'width': 48,\n",
       "     'height': 48},\n",
       "    {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_64.png',\n",
       "     'width': 64,\n",
       "     'height': 64},\n",
       "    {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_128.png',\n",
       "     'width': 128,\n",
       "     'height': 128}],\n",
       "   'icon_width': 512,\n",
       "   'static_icon_width': 512,\n",
       "   'start_date': None,\n",
       "   'is_enabled': True,\n",
       "   'awardings_required_to_grant_benefits': None,\n",
       "   'description': \"Shows the Silver Award... and that's it.\",\n",
       "   'end_date': None,\n",
       "   'sticky_duration_seconds': None,\n",
       "   'subreddit_coin_reward': 0,\n",
       "   'count': 1,\n",
       "   'static_icon_height': 512,\n",
       "   'name': 'Silver',\n",
       "   'resized_static_icons': [{'url': 'https://www.redditstatic.com/gold/awards/icon/silver_16.png',\n",
       "     'width': 16,\n",
       "     'height': 16},\n",
       "    {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_32.png',\n",
       "     'width': 32,\n",
       "     'height': 32},\n",
       "    {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_48.png',\n",
       "     'width': 48,\n",
       "     'height': 48},\n",
       "    {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_64.png',\n",
       "     'width': 64,\n",
       "     'height': 64},\n",
       "    {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_128.png',\n",
       "     'width': 128,\n",
       "     'height': 128}],\n",
       "   'icon_format': None,\n",
       "   'icon_height': 512,\n",
       "   'penny_price': None,\n",
       "   'award_type': 'global',\n",
       "   'static_icon_url': 'https://www.redditstatic.com/gold/awards/icon/silver_512.png'}],\n",
       " 'awarders': [],\n",
       " 'media_only': False,\n",
       " 'can_gild': False,\n",
       " 'spoiler': False,\n",
       " 'locked': False,\n",
       " 'author_flair_text': None,\n",
       " 'treatment_tags': [],\n",
       " 'visited': False,\n",
       " 'removed_by': None,\n",
       " 'num_reports': None,\n",
       " 'distinguished': None,\n",
       " 'subreddit_id': 't5_2qh13',\n",
       " 'author_is_blocked': False,\n",
       " 'mod_reason_by': None,\n",
       " 'removal_reason': None,\n",
       " 'link_flair_background_color': '',\n",
       " 'id': 'y1ppwm',\n",
       " 'is_robot_indexable': True,\n",
       " 'report_reasons': None,\n",
       " 'author': 'Sxzym',\n",
       " 'discussion_type': None,\n",
       " 'num_comments': 2448,\n",
       " 'send_replies': False,\n",
       " 'whitelist_status': 'all_ads',\n",
       " 'contest_mode': False,\n",
       " 'mod_reports': [],\n",
       " 'author_patreon_flair': False,\n",
       " 'author_flair_text_color': None,\n",
       " 'permalink': '/r/worldnews/comments/y1ppwm/russia_blew_up_about_a_third_of_ukraines_energy/',\n",
       " 'parent_whitelist_status': 'all_ads',\n",
       " 'stickied': False,\n",
       " 'url': 'https://news.yahoo.com/russia-blew-third-ukraines-energy-193559881.html',\n",
       " 'subreddit_subscribers': 29902750,\n",
       " 'created_utc': 1665536237.0,\n",
       " 'num_crossposts': 4,\n",
       " 'media': None,\n",
       " 'is_video': False}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "the_json['data']['children'][0]['data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d9d23376",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(the_json['data']['children'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4d7436dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'t3_y19evb'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "the_json['data']['after']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9298bb55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['t3_y1ppwm',\n",
       " 't3_y1qp57',\n",
       " 't3_y1pova',\n",
       " 't3_y1cyrs',\n",
       " 't3_y1eto7',\n",
       " 't3_y1obug',\n",
       " 't3_y1no1f',\n",
       " 't3_y1kbyv',\n",
       " 't3_y1nwjh',\n",
       " 't3_y1n0hh',\n",
       " 't3_y1o7uh',\n",
       " 't3_y1p7a9',\n",
       " 't3_y1o6oe',\n",
       " 't3_y1mth3',\n",
       " 't3_y1njtd',\n",
       " 't3_y1m02c',\n",
       " 't3_y1hqt2',\n",
       " 't3_y1ladv',\n",
       " 't3_y193k4',\n",
       " 't3_y16kya',\n",
       " 't3_y1fqao',\n",
       " 't3_y1p0r1',\n",
       " 't3_y18tkr',\n",
       " 't3_y1p6so',\n",
       " 't3_y19evb']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[post['data']['name'] for post in the_json['data']['children']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f44af2ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "param = {'after':'t3_xz3tn3'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "619ceb17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Response [200]>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "requests.get(url,params =param, headers= headers )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c6ab719f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The function 'reddit_to_csv' will take three arguments: \n",
    "# 1. the subreddit being scraped; 2. the filename, or the name\n",
    "# the csv file will be given; and 3. the number of requests \n",
    "# the user would like to make of reddit's API. \n",
    "\n",
    "def reddit_to_csv(subreddit, filename, n_requests=1):\n",
    "    \n",
    "    #Create an empty list to be used later in function:\n",
    "    posts = []\n",
    "    \n",
    "    #Create User-Agent to avoid 429 res.status_code:\n",
    "    headers = {'User-agent':'Eye eye bot 00'}\n",
    "    \n",
    "    #Establish that 'after' (a variable used later) is None type:\n",
    "    after = None\n",
    "    \n",
    "    #for loop n_requests iterations (n_requests is established by user):\n",
    "    for i in range(n_requests):\n",
    "        print(i)\n",
    "        \n",
    "        if after == None:\n",
    "            params = {}\n",
    "        else:\n",
    "            params = {'after': after}\n",
    "        #Assign 'url' to reddit's base url, plus whatever subreddit \n",
    "        #the user provides,plus .json for clean results:\n",
    "        #url = 'https://www.reddit.com/hot.json'\n",
    "        url = 'https://www.reddit.com/' + str(subreddit) + '/.json'\n",
    "        \n",
    "        #Set my res variable equal to the results from requests.get, \n",
    "        #and the parameters set above like 'url' or 'params':\n",
    "        res = requests.get(url,params=params,headers=headers)\n",
    "        \n",
    "        #Conditional statement to ensure access to the API is approved:\n",
    "        if res.status_code ==200:\n",
    "            the_json = res.json()\n",
    "            \n",
    "            for x in range(len(the_json['data']['children'])):\n",
    "                \n",
    "                #Create temporary dictionary to add results of each post to:\n",
    "                temp_dict = {}\n",
    "                #After looking through the json results, I've selected the below information about the posts\n",
    "                #as those that can potentially add value to my model's results.\n",
    "                temp_dict['subreddit'] = the_json['data']['children'][x]['data']['subreddit']\n",
    "                temp_dict['title'] = the_json['data']['children'][x]['data']['title']\n",
    "                temp_dict['post_paragraph'] = the_json['data']['children'][x]['data']['selftext']\n",
    "                temp_dict['clicked'] = the_json['data']['children'][x]['data']['clicked']\n",
    "                temp_dict['ups'] = the_json['data']['children'][x]['data']['ups']\n",
    "                temp_dict['downs'] = the_json['data']['children'][x]['data']['downs']\n",
    "                temp_dict['likes'] = the_json['data']['children'][x]['data']['likes']\n",
    "                temp_dict['category'] = the_json['data']['children'][x]['data']['category']\n",
    "                temp_dict['number_of_comments'] = the_json['data']['children'][x]['data']['num_comments']\n",
    "                temp_dict['score'] = the_json['data']['children'][x]['data']['score']\n",
    "                temp_dict['author_flair_css_class'] = the_json['data']['children'][x]['data']['author_flair_css_class']\n",
    "                temp_dict['subreddit_type'] = the_json['data']['children'][x]['data']['subreddit_type']\n",
    "                \n",
    "                #Add the temporary dictionary to 'posts',the list of each post's dictionary of information:\n",
    "                posts.append(temp_dict)\n",
    "                #posts.extend(the_json['data']['children'])\n",
    "            after = the_json['data']['after']\n",
    "            \n",
    "        else:\n",
    "            print(res.status_code)\n",
    "            break\n",
    "        time.sleep(1)\n",
    "        \n",
    "    #Turn the list of post dictionaries into a pandas DataFrame:\n",
    "    posts_df = pd.DataFrame(posts)\n",
    "    \n",
    "    #Drop any duplicate rows that may have been pulled:\n",
    "    posts_df.drop_duplicates(inplace = True)\n",
    "    \n",
    "    #Rearrange the columns into a more logical order:\n",
    "    posts_df = posts_df[['subreddit', 'title', 'clicked', 'ups', 'downs', 'post_paragraph', 'likes', 'number_of_comments', 'category', 'score', 'author_flair_css_class', 'subreddit_type']]\n",
    "    \n",
    "    #Save the DataFrame as a .csv file:\n",
    "    posts_df.to_csv(str(filename), index = False, sep = \",\")\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da74a527",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n"
     ]
    }
   ],
   "source": [
    "#Load and save the data as CSV\n",
    "reddit_to_csv(subreddit = 'r/Mario/',\n",
    "              n_requests = 150,\n",
    "              filename = 'mario_reddit_posts.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f706c78f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load and save the data as CSV\n",
    "reddit_to_csv(subreddit = 'r/zelda',\n",
    "              n_requests = 150,\n",
    "              filename = 'zelda_reddit_posts.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20f69c15",
   "metadata": {},
   "outputs": [],
   "source": [
    "mario_df = pd.read_csv('./mario_reddit_posts.csv')\n",
    "mario_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80cb9116",
   "metadata": {},
   "outputs": [],
   "source": [
    "mario_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3e3f0ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "zelda_df = pd.read_csv('./zelda_reddit_posts.csv')\n",
    "zelda_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b64ac72",
   "metadata": {},
   "outputs": [],
   "source": [
    "zelda_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ab98ace",
   "metadata": {},
   "source": [
    "**Append the Mario and Zelda files**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2242d2e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = mario_df.append(zelda_df, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dd6bc0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([mario_df,zelda_df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2b460db",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81eef94e",
   "metadata": {},
   "source": [
    "**CSV CHECKPOINT (check the data again).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1596b325",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "720e8b8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af98a45a",
   "metadata": {},
   "outputs": [],
   "source": [
    "zelda_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7d1a329",
   "metadata": {},
   "outputs": [],
   "source": [
    "mario_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "509af679",
   "metadata": {},
   "outputs": [],
   "source": [
    "zelda_df.shape[0]+mario_df.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f3b2a62",
   "metadata": {},
   "source": [
    "**Create a 'target' column (will equal 1 if the post's subreddit is Mario Party, and 0 if the post's subreddit is Smash Bros. Ultimate):**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5556bdfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['target'] = np.where(df['subreddit'] == 'Mario', 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a84a261",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acc64afe",
   "metadata": {},
   "source": [
    "**Look for columns that don't have any values and can be dropped**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e8d4d58",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['likes'].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bedd0e93",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['category'].isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d584d6e3",
   "metadata": {},
   "source": [
    "**The column 'clicked' is not empty, but the column values are purely False, therefore I will drop 'clicked' as well. The same for columns 'downs' and 'subreddit_type' which are purely 0's and 'public', respectively.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee81a4e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['clicked'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a90ee0ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['downs'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4417a8a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['subreddit_type'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6224d545",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_drop_list = ['likes', 'category', 'clicked', 'downs', 'subreddit_type']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edc94c0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(df_drop_list, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcb55e8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dff78f36",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('master_df.csv', index=False, sep=\",\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "487fffe6",
   "metadata": {},
   "source": [
    "**Set Tokenizer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "135d5f3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = RegexpTokenizer(r'\\w+|\\$[\\d\\.]+|\\S+')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb0654a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b0bed63",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "63a953a4",
   "metadata": {},
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "003583df",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./master_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "133068b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c61aa2c2",
   "metadata": {},
   "source": [
    "**Create function that takes a column containing text and returns the lemmatized version in a new 'cleaned' column**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab89d096",
   "metadata": {},
   "outputs": [],
   "source": [
    "def column_cleaner(column, df=df):\n",
    "    #For some reason, I was running into errors trying to run this code until I added the code\n",
    "    #below (df[column+'_clean'] = \"\"), establishing from the beginning that the new column to be created\n",
    "    #exists in the dataframe and contains nothing but empty strings.\n",
    "    \n",
    "    df[column+'_clean'] = \"\"\n",
    "    \n",
    "    #for loop through each row in the column:\n",
    "    for i in range(len(df[column])):\n",
    "        \n",
    "        #Tokenize, or separate, each word in column's string into its own string (prep for lemmatization):\n",
    "        col_tok = []\n",
    "        col_tok.extend(tokenizer.tokenize(df[column][i].lower()))\n",
    "        col_token = []\n",
    "        [col_token.append(s) for s in col_tok if s not in col_token]\n",
    "        \n",
    "        #Lemmatize the words (cut the word to its base/root, for improved model results):\n",
    "        col_lem = []\n",
    "        for x in range(len(col_token)):\n",
    "            col_lem.append(lemmatizer.lemmatize(col_token[x]))\n",
    "        \n",
    "        #Remove characters and numbers (for improved model results, hopefully):\n",
    "        letters_only_col = []\n",
    "        for c in range(len(col_lem)):\n",
    "            letters_only_col.append(re.sub(\"[^a-zA-Z]\", \"\", col_lem[c]))\n",
    "        \n",
    "        #Remove stopwords (for improved model results):\n",
    "        col_words = [w for w in letters_only_col if not w in stopwords.words('english')]\n",
    "        \n",
    "        #Remove 'cheat' words (words that are in the subreddit's name and also in the column)\n",
    "                       # Mario cheatwords\n",
    "        cheat_words = ['mario', 'party', 'marioparty', 'smash', 'bros', 'browser',\n",
    "                       'ultimate', 'smashbrosultimate', 'super', 'superstar',\n",
    "                       'luigi', 'peach', 'toad', \n",
    "                       \n",
    "                       # Zelda cheatwords\n",
    "                       \"zelda\", \"princess\", \"link\",\"botw\", \"totk\", \"oc\",\n",
    "                       'majora', 'mask', 'ocarina time', \"mm\", \n",
    "                       \n",
    "                       # Mario Movie\n",
    "                       \"movie\", \"trailer\", \"chris\", \"pratt\", \"voice\", \"poster\"] \n",
    "        col_words = [w for w in letters_only_col if not w in cheat_words]\n",
    "        \n",
    "        #Ensure that there are no 'None' objects in title_words:\n",
    "        col_words = list(filter(None, col_words))\n",
    "\n",
    "        #Join the lemmatized words - stopwords back to one long string (prep for\n",
    "        #vectorization, done outside/after this function):\n",
    "        col_words = \" \".join(col_words)\n",
    "\n",
    "        #Fill new column with 'cleaned' string from column:\n",
    "        df.loc[i,(column+'_clean')] = col_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b48d51bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43b26fe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "column_cleaner(column='title', df=df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aa5123a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df['post_paragraph'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13c408c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df['post_paragraph'] = df['post_paragraph'].replace(np.nan, \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f70cfe68",
   "metadata": {},
   "outputs": [],
   "source": [
    "#column_cleaner(column='post_paragraph', df=df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7596b244",
   "metadata": {},
   "source": [
    "**Save version of DataFrame**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f35de8f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('master_df_cleaned.csv', index=False, sep=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20141be9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2501a4f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sample(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5cc5d7b",
   "metadata": {},
   "source": [
    "**Create CountVectorize Function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3723adcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_vec_column(column, func_df=df):\n",
    "    #Instantiate CountVectorizer:\n",
    "    vect = CountVectorizer()\n",
    "    \n",
    "    #Create temporary variable X_text that takes on the fit/transformed results of the column:\n",
    "    X_text = vect.fit_transform(func_df[column])\n",
    "    \n",
    "    #Turn X_text into an array (prep to easily make a DataFrame):\n",
    "    X_text = X_text.toarray()\n",
    "    \n",
    "    #Create a temporary DataFrame with each word/word-pair/word-group as the columns:\n",
    "    temp_df = pd.DataFrame(X_text,\n",
    "                           columns=vect.get_feature_names())\n",
    "    \n",
    "    #Add the original column name to the beginning of the new columns' names to differentiate from which column\n",
    "    # the vectorized words came from (this may impact the strength of the model):\n",
    "    for i in range(len(temp_df.columns)):\n",
    "        #print(i)\n",
    "        temp_df.rename(columns={temp_df.columns[i]: column + '_' + temp_df.columns[i]}, inplace=True)\n",
    "    \n",
    "    #Combine the two DataFrames:\n",
    "    func_df = pd.concat([func_df, temp_df], axis=1, join_axes=[func_df.index])\n",
    "    return func_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61795514",
   "metadata": {},
   "source": [
    "**Create TF-IDF Function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23e00c74",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfidf_column(column, func_df=df):\n",
    "    #Instantiate TfidfVectorizer:\n",
    "    tfidf_vect = TfidfVectorizer()\n",
    "    \n",
    "    #Create temporary variable X_text that takes on the fit/transformed results of the column:\n",
    "    X_text = tfidf_vect.fit_transform(func_df[column])\n",
    "    \n",
    "    #Turn X_text into an array (prep to easily make a DataFrame):\n",
    "    X_text = X_text.toarray()\n",
    "    \n",
    "    #Create a temporary DataFrame with each word/word-pair/word-group as the columns:\n",
    "    temp_df = pd.DataFrame(X_text,\n",
    "                           columns=tfidf_vect.get_feature_names())\n",
    "    \n",
    "    #Add the original column name to the beginning of the new columns' names to differentiate from which column\n",
    "    # the tf-idf vectorized words came from (this may impact the strength of the model):\n",
    "    for i in range(len(temp_df.columns)):\n",
    "        #print(i)\n",
    "        temp_df.rename(columns={temp_df.columns[i]: column + '_' + temp_df.columns[i]}, inplace=True)\n",
    "    \n",
    "    #Combine the two DataFrames:\n",
    "    func_df = pd.concat([func_df, temp_df], axis=1, join_axes=[func_df.index])\n",
    "    return func_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad2f3c27",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2eb7a7e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8376aea9",
   "metadata": {},
   "source": [
    "### Top 25 word for Mario Game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "242149c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nplot(subreddit,column):\n",
    "    fig,ax = plt.subplots(1,3, figsize=(15,6))\n",
    "    fig.suptitle(\"Top 25 Most Common Words of \" + subreddit, fontsize=20)\n",
    "    geb={} # Collet top 20 word of each n-gram\n",
    "\n",
    "    for i in range(1,4): # common bigram trigram quadgram\n",
    "        word = []\n",
    "    \n",
    "        cvec = CountVectorizer(ngram_range=(i,i), stop_words=\"english\")\n",
    "        text_cvec = cvec.fit_transform(df[df['subreddit']== subreddit ][column])\n",
    "    \n",
    "        vec = pd.DataFrame(text_cvec.toarray(),columns= cvec.get_feature_names_out())\n",
    "        word.append(vec.sum().sort_values(ascending=False)[:25].index)\n",
    "        geb[i] = list(word[0])\n",
    "        ax[i-1].barh(vec.sum().sort_values(ascending=False)[:25][::-1].index,\n",
    "                 vec.sum().sort_values(ascending=False)[:25][::-1])\n",
    "        ax[i-1].set_xlabel(\"number of word\")\n",
    "        ax[i-1].set_ylabel(\"word important\")\n",
    "        ax[i-1].set_title(f\" {i}-grams\")\n",
    "        plt.tight_layout();\n",
    "\n",
    "    \n",
    "    print(f\"Word Important of each n_gram:\")\n",
    "    for x, y in geb.items():\n",
    "        print(f\"word {x}-gram\", y)\n",
    "        print(\"------------------------------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b33d3b4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "nplot(\"Mario\",\"title\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33a17ce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "nplot(\"Mario\",\"title_clean\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed7d7868",
   "metadata": {},
   "source": [
    "### Top 25 word for Zelda Game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa2520f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "nplot(\"zelda\",\"title\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ea6e85f",
   "metadata": {},
   "outputs": [],
   "source": [
    "nplot(\"zelda\",\"title_clean\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9014166e",
   "metadata": {},
   "source": [
    "### Messy Title Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "575da401",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[\"title\"]\n",
    "y = df[\"target\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b7e951f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Redefine training and testing sets.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,\n",
    "                                                    y,\n",
    "                                                    test_size=0.20,\n",
    "                                                    stratify=y,\n",
    "                                                    random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46d9852e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c838501c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's set a pipeline up with two stages:\n",
    "# 1. CountVectorizer (transformer)\n",
    "# 2. Multinomial Naive Bayes (estimator)\n",
    "\n",
    "pipe = Pipeline([\n",
    "    (\"cvec\", CountVectorizer()), # Transformer (fit, transform)\n",
    "    (\"nb\", MultinomialNB()) # Estimator or model (fit, predict)    \n",
    "])\n",
    "\n",
    "# .predict() of MultinomialNB allows us to have a score to judge\n",
    "# our hyperparameters combinations when GridSearching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91c1a58f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search over the following values of hyperparameters:\n",
    "# Maximum number of features fit: 2000, 3000, 4000, 5000\n",
    "# Minimum number of documents needed to include token: 2, 3\n",
    "# Maximum number of documents needed to include token: 90%, 95%\n",
    "# Check (individual tokens) and also check (individual tokens and 2-grams).\n",
    "# Check Stop_words: None or English.\n",
    "\n",
    "pipe_params = {\n",
    "    \"cvec__max_features\":[2000,3000,4000,5000],\n",
    "    \"cvec__min_df\": [2,3],\n",
    "    \"cvec__max_df\": [0.9,0.95],\n",
    "    \"cvec__ngram_range\": [(1,1),(1,2)],\n",
    "    \"cvec__stop_words\": [None,\"english\"]\n",
    "}\n",
    "\n",
    "# ngram_range of (1,1) just returns individual tokens\n",
    "# ngram_range of (1,2) just returns individual tokens AND bi-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98a17582",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate GridSearchCV.\n",
    "\n",
    "gs = GridSearchCV(pipe, # what object are we optimizing?\n",
    "                  param_grid=pipe_params, # what parameters values are we searching?\n",
    "                  cv=5) # 5-fold cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fee86572",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit GridSearch to training data.\n",
    "start_time = time.time()\n",
    "gs.fit(X_train,y_train)\n",
    "print(f\"Runtime:{time.time()-start_time}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "204337f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# What's the best score?\n",
    "gs.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0c0dd03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# What's the best params?\n",
    "gs.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fb16f66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Score model on training set.\n",
    "# What is the score on a classifier? Accuracy\n",
    "gs.score(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aefa341",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Score model on testing set.\n",
    "gs.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d175d81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the transformer.\n",
    "tvec = TfidfVectorizer(stop_words=\"english\")\n",
    "\n",
    "# convert training data to dataframe\n",
    "X_train_df = pd.DataFrame(tvec.fit_transform(X_train).todense(), \n",
    "                          columns=tvec.get_feature_names_out())\n",
    "# Plot top 20 occuring words\n",
    "X_train_df.sum().sort_values(ascending=True).tail(20).plot(kind='barh')\n",
    "plt.title(\"Top 20 Common Words from both Reddits\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71c7786d",
   "metadata": {},
   "source": [
    "Mario is mention more often than Zelda. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f49addf0",
   "metadata": {},
   "source": [
    "## Cleaned Title Model (Remove \"Cheat words\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fa33ccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[\"title_clean\"]\n",
    "y = df[\"target\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "198ee047",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Redefine training and testing sets.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,\n",
    "                                                    y,\n",
    "                                                    test_size=0.20,\n",
    "                                                    stratify=y,\n",
    "                                                    random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0683f4c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8e1950e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's set a pipeline up with two stages:\n",
    "# 1. CountVectorizer (transformer)\n",
    "# 2. Multinomial Naive Bayes (estimator)\n",
    "\n",
    "pipe = Pipeline([\n",
    "    (\"cvec\", CountVectorizer()), # Transformer (fit, transform)\n",
    "    (\"nb\", MultinomialNB()) # Estimator or model (fit, predict)    \n",
    "])\n",
    "\n",
    "# .predict() of MultinomialNB allows us to have a score to judge\n",
    "# our hyperparameters combinations when GridSearching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da5754c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search over the following values of hyperparameters:\n",
    "# Maximum number of features fit: 2000, 3000, 4000, 5000\n",
    "# Minimum number of documents needed to include token: 2, 3\n",
    "# Maximum number of documents needed to include token: 90%, 95%\n",
    "# Check (individual tokens) and also check (individual tokens and 2-grams).\n",
    "# Check Stop_words: None or English.\n",
    "\n",
    "pipe_params = {\n",
    "    \"cvec__max_features\":[2000,3000,4000,5000],\n",
    "    \"cvec__min_df\": [2,3],\n",
    "    \"cvec__max_df\": [0.9,0.95],\n",
    "    \"cvec__ngram_range\": [(1,1),(1,2)],\n",
    "    \"cvec__stop_words\": [None,\"english\"]\n",
    "}\n",
    "\n",
    "# ngram_range of (1,1) just returns individual tokens\n",
    "# ngram_range of (1,2) just returns individual tokens AND bi-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9f9cb62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate GridSearchCV.\n",
    "\n",
    "gs = GridSearchCV(pipe, # what object are we optimizing?\n",
    "                  param_grid=pipe_params, # what parameters values are we searching?\n",
    "                  cv=5) # 5-fold cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1177e6a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit GridSearch to training data.\n",
    "start_time = time.time()\n",
    "gs.fit(X_train,y_train)\n",
    "print(f\"Runtime:{time.time()-start_time}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ad36f42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# What's the best score?\n",
    "gs.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd67dfaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# What's the best params?\n",
    "gs.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f747520d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Score model on training set.\n",
    "# What is the score on a classifier? Accuracy\n",
    "gs.score(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e0d0d29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Score model on testing set.\n",
    "gs.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e41f58f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define score model dataframe for comparison\n",
    "list_of_rows = []\n",
    "\n",
    "def modelscore(gsmodel,transformer,model,cheatword):\n",
    "    s = pd.Series({\"Transformer\": transformer, #\"CountVectorizer\",\"Tfid\" \n",
    "               \"Model\" : model, #\"MultinomialNB\",\"Logistic\"\n",
    "               \"Cheat Word\" : cheatword , #\"Included\" , \"Excluded\"\n",
    "               \"Train Score\" : gsmodel.score(X_train,y_train),\n",
    "               \"Test Score\" : gsmodel.score(X_test,y_test)\n",
    "               })\n",
    "    print(s)\n",
    "    list_of_rows.append(s)\n",
    "    global score_df \n",
    "    score_df = pd.DataFrame(list_of_rows)\n",
    "score_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f593ba07",
   "metadata": {},
   "outputs": [],
   "source": [
    "modelscore(gs, \"CountVectorizer\", \"MultinomialNB\", \"Excluded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d9177de",
   "metadata": {},
   "outputs": [],
   "source": [
    "score_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4896d050",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's set a pipeline up with two stages:\n",
    "# 1. TfidfVectorizer (transformer)\n",
    "# 2. Multinomial Naive Bayes (estimator)\n",
    "\n",
    "pipe_tvec = Pipeline([\n",
    "    (\"tvec\", TfidfVectorizer()), # Transformer (fit, transform)\n",
    "    (\"nb\", MultinomialNB()) # Estimator or model (fit, predict)    \n",
    "])\n",
    "\n",
    "# .predict() of MultinomialNB allows us to have a score to judge\n",
    "# our hyperparameters combinations when GridSearching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a822bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search over the following values of hyperparameters:\n",
    "# Maximum number of features fit: 2000, 3000, 4000, 5000\n",
    "# No stop words and english stop words\n",
    "# Check (individual tokens) and also check (individual tokens and 2-grams).\n",
    "\n",
    "pipe_tvec_params = {\"tvec__max_features\" : [2000,3000,4000,5000],\n",
    "                    \"tvec__min_df\": [2,3],\n",
    "                    \"tvec__max_df\": [0.9,0.95],\n",
    "                    \"tvec__stop_words\" : [None, \"english\"],\n",
    "                    \"tvec__ngram_range\" : [(1,1),(1,2)]\n",
    "                  }\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2009f748",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate GridSearchCV.\n",
    "gs_tvec = GridSearchCV(estimator=pipe_tvec,\n",
    "                      param_grid=pipe_tvec_params,\n",
    "                      cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10acedfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit GridSearch to training data.\n",
    "gs_tvec.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3471c1a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_tvec.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b72c6e2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Score model on training set.\n",
    "gs_tvec.score(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55e8583a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Score model on testing set.\n",
    "gs_tvec.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf83a9f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "modelscore(gs_tvec, \"TfidfVectorizer\", \"MultinomialNB\", \"Excluded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce9cd684",
   "metadata": {},
   "source": [
    "### Find top occuring words after remove game name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c612ab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the transformer.\n",
    "tvec = TfidfVectorizer(stop_words=\"english\")\n",
    "\n",
    "# convert training data to dataframe\n",
    "X_train_df = pd.DataFrame(tvec.fit_transform(X_train).todense(), \n",
    "                          columns=tvec.get_feature_names_out())\n",
    "# Plot top 20 occuring words\n",
    "X_train_df.sum().sort_values(ascending=True).tail(20).plot(kind='barh')\n",
    "plt.title(\"Top 20 Common Words after removing game name\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "153c082e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb6c9f47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's set a pipeline up with two stages:\n",
    "# 1. CountfVectorizer (transformer)\n",
    "# 2. LogisticRegression (estimator)\n",
    "\n",
    "pipe_cvec_log = Pipeline([\n",
    "    (\"cvec\", CountVectorizer()), # Transformer (fit, transform)\n",
    "    (\"logreg\", LogisticRegression()) # Estimator or model (fit, predict)    \n",
    "])\n",
    "\n",
    "# .predict() of MultinomialNB allows us to have a score to judge\n",
    "# our hyperparameters combinations when GridSearching\n",
    "\n",
    "# Search over the following values of hyperparameters:\n",
    "# Maximum number of features fit: 2000, 3000, 4000, 5000\n",
    "# No stop words and english stop words\n",
    "# Check (individual tokens) and also check (individual tokens and 2-grams).\n",
    "\n",
    "pipe_cvec_log_params = {\"cvec__max_features\":[2000,3000,4000,5000],\n",
    "                        \"cvec__min_df\": [2,3],\n",
    "                        \"cvec__max_df\": [0.9,0.95],\n",
    "                        \"cvec__ngram_range\": [(1,1),(1,2)],\n",
    "                        \"cvec__stop_words\": [None,\"english\"]\n",
    "                       }\n",
    "    \n",
    "\n",
    "# Instantiate GridSearchCV.\n",
    "gs_cvec_log = GridSearchCV(estimator=pipe_cvec_log,\n",
    "                      param_grid=pipe_cvec_log_params,\n",
    "                      cv=5)\n",
    "\n",
    "# Fit GridSearch to training data.\n",
    "gs_cvec_log.fit(X_train,y_train)\n",
    "\n",
    "gs_cvec_log.best_params_\n",
    "\n",
    "# Score model on training set.\n",
    "gs_cvec_log.score(X_train,y_train)\n",
    "\n",
    "# Score model on testing set.\n",
    "gs_cvec_log.score(X_test,y_test)\n",
    "\n",
    "# Save model score to score dataframe\n",
    "modelscore(gs_cvec_log, \"CountVectorizer\", \"Logistic\", \"Excluded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "313bad76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's set a pipeline up with two stages:\n",
    "# 1. TfidVectorizer (transformer)\n",
    "# 2. LogisticRegression (estimator)\n",
    "\n",
    "pipe_tvec_log = Pipeline([\n",
    "    (\"tvec\", TfidfVectorizer()), # Transformer (fit, transform)\n",
    "    (\"logreg\", LogisticRegression()) # Estimator or model (fit, predict)    \n",
    "])\n",
    "\n",
    "# .predict() of MultinomialNB allows us to have a score to judge\n",
    "# our hyperparameters combinations when GridSearching\n",
    "\n",
    "# Search over the following values of hyperparameters:\n",
    "# Maximum number of features fit: 2000, 3000, 4000, 5000\n",
    "# No stop words and english stop words\n",
    "# Check (individual tokens) and also check (individual tokens and 2-grams).\n",
    "\n",
    "pipe_tvec_log_params = {\"tvec__max_features\" : [2000,3000,4000,5000],\n",
    "                        \"tvec__min_df\": [2,3],\n",
    "                        \"tvec__max_df\": [0.9,0.95],\n",
    "                        \"tvec__stop_words\" : [None, \"english\"],\n",
    "                        \"tvec__ngram_range\" : [(1,1),(1,2)]\n",
    "                       }\n",
    "    \n",
    "\n",
    "# Instantiate GridSearchCV.\n",
    "gs_tvec_log = GridSearchCV(estimator=pipe_tvec_log,\n",
    "                      param_grid=pipe_tvec_log_params,\n",
    "                      cv=5)\n",
    "\n",
    "# Fit GridSearch to training data.\n",
    "gs_tvec_log.fit(X_train,y_train)\n",
    "\n",
    "gs_tvec_log.best_params_\n",
    "\n",
    "# Score model on training set.\n",
    "gs_tvec_log.score(X_train,y_train)\n",
    "\n",
    "# Score model on testing set.\n",
    "gs_tvec_log.score(X_test,y_test)\n",
    "\n",
    "# Save model score to score dataframe\n",
    "modelscore(gs_tvec_log, \"TfidfVectorizer\", \"Logistic\", \"Excluded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "655dd418",
   "metadata": {},
   "outputs": [],
   "source": [
    "score_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33ec555d",
   "metadata": {},
   "source": [
    "**Error Analysis**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8b5f366",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check Misprediction"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
