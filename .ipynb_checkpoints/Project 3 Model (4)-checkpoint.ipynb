{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d7e99191",
   "metadata": {},
   "source": [
    "# Project 3: Reddit API NLP Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bebd7fb",
   "metadata": {},
   "source": [
    "## Problem Statement"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc384739",
   "metadata": {},
   "source": [
    "Being part of the Nintendo Marketing Team of nintendo switch, I am interested in finding out what popular topics and keyword jargons belong to the fields of Mario and Legend of Zelda. Conducting analysis on Reddit posts will allow me to craft online content and advertisements to better target people interested in casual, adventure, platformer video games.\n",
    "\n",
    "The main objective of this project is to scrape two subreddits: `r/Mario` and `r/Zelda` using Reddit's API. The scraped data from the two subreddits will then be passed through various classification models, `CountVectorizer`/`TfidVectorizer` with `Naive Bayes Classifier` and `LogisticRegression` that will assign each observation to the most likely class of subreddit. The models should help the data science marketing team of my company identify what makes the respective subreddit posts unique from one another.\n",
    "\n",
    "In this process, the subreddit posts will undergo preprocessing and EDA. The success of the models that we decide on will be determined through the highest accuracy based on the scores obtained."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "244b1177",
   "metadata": {},
   "source": [
    "## Executive Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b5399b4",
   "metadata": {},
   "source": [
    "Natural Language Processing, or NLP for short, involves using specialized machine learning techniques to make predictions about the text in a body of documents, including things like authorship attribution, sentiment analysis, text generation, and in some cases the appearance of something resembling semantic understanding.\n",
    "\n",
    "Nintendo Switch, a hybrid home console and handheld device is one of highest revenue generator of Nintendo, it had outsold the lifetime sales of Wii U, its home console predecessor.Mario Kart 8 Deluxe is the best-selling game on the platform at over 46 million copies sold The Mario franchise alone has sold 167.11 million copies on the Nintendo Switch, which is the most the franchise has ever sold on a single platform. The Legend of Zelda has also sold the most on a single platform with the Nintendo Switch with 41.13 million copies. \n",
    "\n",
    "In this project, I would like to further explore the key similarities and differences between Data Science and Software Engineering in terms of the current discussions and topics that people are discussing on Reddit. Reddit, in recent times, have become a popular avenue for people all over the world to ask one another about different career prospects and experiences. As such, scraping Subreddit Posts gives us an interesting source of data that we can analyze to understand what are the popular topics in these respective career fields.\n",
    "\n",
    "The web scraping portion of this project is covered in another notebook. In this notebook, I will be covering the steps taken to clean and analyze the data collected, as well as further steps taken to pre-process the text data, visualize the data, use different models to find the optimal model and analyze misclassified posts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52a2f621",
   "metadata": {},
   "source": [
    "## Contents:\n",
    "- [Data Collection](#Data-Collection) (In notebook \"Data_Collection_Reddit.ipynb\")\n",
    "- [Data Cleaning](#Data-Cleaning)\n",
    "- [Explanatory Data Analysis](#Explanatory-Data-Analysis)\n",
    "- [Pre-Processing](#Pre-Processing)\n",
    "- [Data Visualization](#Data-Visualization)\n",
    "- [Modelling](#Modelling)\n",
    "- [Evaluation](#Evaluation)\n",
    "- [Conclusion](#Conclusion)\n",
    "- [Recommendations](#Recommendations)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8362982d",
   "metadata": {},
   "source": [
    "### Importing Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ced9909",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import requests\n",
    "import time\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import regex as re\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier, RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.naive_bayes import MultinomialNB # NLP classification\n",
    "from sklearn.metrics import confusion_matrix, plot_confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "from wordcloud import WordCloud"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e1e3e8a",
   "metadata": {},
   "source": [
    "I have outlined my process of scraping data in the other Jupyter Notebook in this project folder. In this notebook, I will be reading in the csv files that contain my scrapped data for both the mario and zelda subreddit posts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f996af41",
   "metadata": {},
   "source": [
    "### Read in CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ea6796f",
   "metadata": {},
   "outputs": [],
   "source": [
    "mario = pd.read_csv(\"mario_reddit_posts.csv\")\n",
    "mario.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b442c46",
   "metadata": {},
   "outputs": [],
   "source": [
    "zelda = pd.read_csv(\"zelda_reddit_posts.csv\")\n",
    "zelda.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69a85173",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mario.shape)\n",
    "print(zelda.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "003583df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import combinded df\n",
    "df = pd.read_csv(\"./master_df.csv\")\n",
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef465136",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30e1e391",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overview of Numerical Columns\n",
    "df.groupby(\"subreddit\").mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b09896b9",
   "metadata": {},
   "source": [
    "There is a clear difference of score between the reddits. Mario has less active communities on Reddit with 113k members to Zelda of 2.1m members."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c5f579a",
   "metadata": {},
   "source": [
    "### For Simplicity of the model: We would focus only on the title column of the dataframe. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b73f8119",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[[\"subreddit\",\"target\",\"title\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acccc723",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cb59dbb",
   "metadata": {},
   "source": [
    "### WordCloud Visualisation of Common Word in Mario and Zelda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec197061",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of word\n",
    "text= mario['title']\n",
    "\n",
    "# Create the wordcloud object\n",
    "wordcloud = WordCloud(width=480, height=480, margin=0, \n",
    "                      background_color='white').generate(str(text))\n",
    "\n",
    "# Display the generated image:\n",
    "plt.imshow(wordcloud,interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.margins(x=0, y=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed2a6df4",
   "metadata": {},
   "source": [
    "There is significant interested on the upcoming \"Mario Movie\". This is out of the scope for this analysis. Hence, we would remove word relating to the movie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8598d1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of word\n",
    "text= zelda['title']\n",
    "\n",
    "# Create the wordcloud object\n",
    "wordcloud = WordCloud(width=480, height=480, margin=0, \n",
    "                      background_color='white').generate(str(text))\n",
    "\n",
    "# Display the generated image:\n",
    "plt.imshow(wordcloud,interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.margins(x=0, y=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f323dec1",
   "metadata": {},
   "source": [
    "There is significant mention of the characters' name and game title. This could be considered \"cheat words\" as it is irrelevant to find the characteristic of players."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "487fffe6",
   "metadata": {},
   "source": [
    "**Set Tokenizer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "135d5f3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = RegexpTokenizer(r'\\w+|\\$[\\d\\.]+|\\S+')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb0654a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c61aa2c2",
   "metadata": {},
   "source": [
    "**Create function that takes a column containing text and returns the lemmatized version in a new 'cleaned' column**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab89d096",
   "metadata": {},
   "outputs": [],
   "source": [
    "def column_cleaner(column, df=df):   \n",
    "    #for loop through each row in the column:\n",
    "    for i in range(len(df[column])):\n",
    "        \n",
    "        #Tokenize, or separate, each word in column's string into its own string (prep for lemmatization):\n",
    "        col_tok = []\n",
    "        col_tok.extend(tokenizer.tokenize(df[column][i].lower()))\n",
    "        col_token = []\n",
    "        [col_token.append(s) for s in col_tok if s not in col_token]\n",
    "        \n",
    "        #Lemmatize the words (cut the word to its base/root, for improved model results):\n",
    "        col_lem = []\n",
    "        for x in range(len(col_token)):\n",
    "            col_lem.append(lemmatizer.lemmatize(col_token[x]))\n",
    "        \n",
    "        #Remove characters and numbers (for improved model results, hopefully):\n",
    "        letters_only_col = []\n",
    "        for c in range(len(col_lem)):\n",
    "            letters_only_col.append(re.sub(\"[^a-zA-Z]\", \"\", col_lem[c]))\n",
    "        \n",
    "        #Remove stopwords (for improved model results):\n",
    "        col_words = [w for w in letters_only_col if not w in stopwords.words('english')]\n",
    "        \n",
    "        #Remove 'cheat' words (words that are in the subreddit's name and also in the column)\n",
    "        # Remove characters' name and game title\n",
    "                       # Mario cheatwords\n",
    "        cheat_words = ['mario', 'party', 'marioparty', 'smash', 'bros', 'browser',\n",
    "                       'ultimate', 'smashbrosultimate', 'super', 'superstar',\n",
    "                       'luigi', 'peach', 'toad', 'bowser', 'game', 'ss', 'waluigi',\n",
    "                       'daisy', 'yoshi', 'donkey', 'king boo', 'kong','diddy', \n",
    "                       'rosalina', 'toadette','toadsworth', 'captain', 'poochy',\n",
    "                       'birdo', 'pauline', 'kamek', 'kammy', 'koopa', 'jr',\n",
    "                       'wario', 'fawful', 'kart', 'paper', 'kart', 'sonic', 'dr.',\n",
    "                       \n",
    "                       # Zelda cheatwords\n",
    "                       \"zelda\", \"princess\", \"link\",\"botw\", \"totk\", \"oc\", \"oot\",\n",
    "                       'majora', 'mask', 'ocarina', 'time', \"mm\", \"hyrule\", \"tp\",\n",
    "                       'ganon', 'darklink','nightmares','twinrova','vaati',\n",
    "                       'zant', 'demise', 'yuga', 'kohga', 'master', 'twilight',\n",
    "                       'tears', 'kingdom', 'wind', 'waker', 'phantom', 'hourglass',\n",
    "                       'legend', 'awakening', 'adventure', 'skyward', 'sword',\n",
    "                       'tri',  'force', 'heroes', 'ww', 'breath', 'wild',\n",
    "                       \n",
    "                       # Mario Movie\n",
    "                       \"movie\", \"trailer\", \"chris\", \"pratt\", \"voice\", \"poster\",\n",
    "                       \"sequel\", \"remade\", \"charles\", \"martinet\", \"charlie\",\n",
    "                       \"anya\", \"jack\", \"black\", \"actor\", \"live\", \"action\",\n",
    "                       \"netflix\", \"live\", \"action\", \"cast\"] \n",
    "        col_words = [w for w in letters_only_col if not w in cheat_words]\n",
    "        \n",
    "        #Ensure that there are no 'None' objects in title_words:\n",
    "        col_words = list(filter(None, col_words))\n",
    "\n",
    "        #Join the lemmatized words - stopwords back to one long string (prep for\n",
    "        #vectorization, done outside/after this function):\n",
    "        col_words = \" \".join(col_words)\n",
    "\n",
    "        #Fill new column with 'cleaned' string from column:\n",
    "        df.loc[i,(column+'_clean')] = col_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b48d51bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43b26fe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "column_cleaner(column='title', df=df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7596b244",
   "metadata": {},
   "source": [
    "**Save version of DataFrame**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f35de8f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('master_df_cleaned.csv', index=False, sep=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20141be9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2501a4f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sample(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8376aea9",
   "metadata": {},
   "source": [
    "### Top 25 word for Mario Game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "242149c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nplot(subreddit,column):\n",
    "    fig,ax = plt.subplots(1,3, figsize=(15,6))\n",
    "    fig.suptitle(\"Top 25 Most Common Words of \" + subreddit, fontsize=20)\n",
    "    geb={} # Collet top 25 word of each n-gram\n",
    "\n",
    "    for i in range(1,4): # common bigram trigram quadgram\n",
    "        word = []\n",
    "    \n",
    "        cvec = CountVectorizer(ngram_range=(i,i), stop_words=\"english\")\n",
    "        text_cvec = cvec.fit_transform(df[df['subreddit']== subreddit ][column])\n",
    "    \n",
    "        vec = pd.DataFrame(text_cvec.toarray(),columns= cvec.get_feature_names_out())\n",
    "        word.append(vec.sum().sort_values(ascending=False)[:25].index)\n",
    "        geb[i] = list(word[0])\n",
    "        ax[i-1].barh(vec.sum().sort_values(ascending=False)[:25][::-1].index,\n",
    "                 vec.sum().sort_values(ascending=False)[:25][::-1])\n",
    "        ax[i-1].set_xlabel(\"number of word\")\n",
    "        ax[i-1].set_ylabel(\"word important\")\n",
    "        ax[i-1].set_title(f\" {i}-grams\")\n",
    "        plt.tight_layout();\n",
    "\n",
    "    \n",
    "    print(f\"Word Important of each n_gram:\")\n",
    "    for x, y in geb.items():\n",
    "        print(f\"word {x}-gram\", y)\n",
    "        print(\"------------------------------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b33d3b4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "nplot(\"Mario\",\"title\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33a17ce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove character names, game titles and movie related.\n",
    "nplot(\"Mario\",\"title_clean\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0c4c688",
   "metadata": {},
   "source": [
    "Mario reddit are more focus on the looks of characters and meme. Moreover, there a considerable numbers of design and fan for the franchise."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed7d7868",
   "metadata": {},
   "source": [
    "### Top 25 word for Zelda Game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa2520f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "nplot(\"zelda\",\"title\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ea6e85f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove character names and game titles.\n",
    "nplot(\"zelda\",\"title_clean\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bd56d84",
   "metadata": {},
   "source": [
    "Zelda reddit are more focus on the art with community led event of [linktober](https://www.linktober.com/). Moreover, there a considerable numbers of discussion on gameplay specific topics such as boss event, seasonal event and story."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9014166e",
   "metadata": {},
   "source": [
    "### Messy Title Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "575da401",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[\"title\"]\n",
    "y = df[\"target\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b7e951f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Redefine training and testing sets.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,\n",
    "                                                    y,\n",
    "                                                    test_size=0.20,\n",
    "                                                    stratify=y,\n",
    "                                                    random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46d9852e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c838501c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's set a pipeline up with two stages:\n",
    "# 1. CountVectorizer (transformer)\n",
    "# 2. Multinomial Naive Bayes (estimator)\n",
    "\n",
    "pipe = Pipeline([\n",
    "    (\"cvec\", CountVectorizer()), # Transformer (fit, transform)\n",
    "    (\"nb\", MultinomialNB()) # Estimator or model (fit, predict)    \n",
    "])\n",
    "\n",
    "# .predict() of MultinomialNB allows us to have a score to judge\n",
    "# our hyperparameters combinations when GridSearching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91c1a58f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search over the following values of hyperparameters:\n",
    "# Maximum number of features fit: 500, 1000, 1500, 2000\n",
    "# Minimum number of documents needed to include token: 2, 3\n",
    "# Maximum number of documents needed to include token: 90%, 95%\n",
    "# Check (individual tokens) and also check (individual tokens and 2-grams).\n",
    "# Check Stop_words: None or English.\n",
    "\n",
    "pipe_params = {\n",
    "    \"cvec__max_features\":[500,1000,1500,2000],\n",
    "    \"cvec__min_df\": [2,3],\n",
    "    \"cvec__max_df\": [0.9,0.95],\n",
    "    \"cvec__ngram_range\": [(1,1),(1,2)],\n",
    "    \"cvec__stop_words\": [None,\"english\"]\n",
    "}\n",
    "\n",
    "# ngram_range of (1,1) just returns individual tokens\n",
    "# ngram_range of (1,2) just returns individual tokens AND bi-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98a17582",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate GridSearchCV.\n",
    "\n",
    "gs = GridSearchCV(pipe, # what object are we optimizing?\n",
    "                  param_grid=pipe_params, # what parameters values are we searching?\n",
    "                  cv=5) # 5-fold cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fee86572",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit GridSearch to training data.\n",
    "start_time = time.time()\n",
    "gs.fit(X_train,y_train)\n",
    "print(f\"Runtime:{time.time()-start_time}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "204337f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# What's the best score?\n",
    "gs.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0c0dd03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# What's the best params?\n",
    "gs.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fb16f66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Score model on training set.\n",
    "# What is the score on a classifier? Accuracy\n",
    "gs.score(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aefa341",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Score model on testing set.\n",
    "gs.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d175d81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the transformer.\n",
    "tvec = TfidfVectorizer(stop_words=\"english\")\n",
    "\n",
    "# convert training data to dataframe\n",
    "X_train_df = pd.DataFrame(tvec.fit_transform(X_train).todense(), \n",
    "                          columns=tvec.get_feature_names_out())\n",
    "# Plot top 20 occuring words\n",
    "X_train_df.sum().sort_values(ascending=True).tail(20).plot(kind='barh')\n",
    "plt.title(\"Top 20 Common Words from both Reddits\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71c7786d",
   "metadata": {},
   "source": [
    "Mario is mention more often than Zelda. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f49addf0",
   "metadata": {},
   "source": [
    "## Cleaned Title Model (Remove \"Cheat words\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fa33ccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[\"title_clean\"]\n",
    "y = df[\"target\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "198ee047",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Redefine training and testing sets.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,\n",
    "                                                    y,\n",
    "                                                    test_size=0.20,\n",
    "                                                    stratify=y,\n",
    "                                                    random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0683f4c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8e1950e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's set a pipeline up with two stages:\n",
    "# 1. CountVectorizer (transformer)\n",
    "# 2. Multinomial Naive Bayes (estimator)\n",
    "\n",
    "pipe = Pipeline([\n",
    "    (\"cvec\", CountVectorizer()), # Transformer (fit, transform)\n",
    "    (\"nb\", MultinomialNB()) # Estimator or model (fit, predict)    \n",
    "])\n",
    "\n",
    "# .predict() of MultinomialNB allows us to have a score to judge\n",
    "# our hyperparameters combinations when GridSearching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da5754c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search over the following values of hyperparameters:\n",
    "# Maximum number of features fit: 500, 1000, 1500, 2000\n",
    "# Minimum number of documents needed to include token: 2, 3\n",
    "# Maximum number of documents needed to include token: 90%, 95%\n",
    "# Check (individual tokens) and also check (individual tokens and 2-grams).\n",
    "# Check Stop_words: None or English.\n",
    "\n",
    "pipe_params = {\n",
    "    \"cvec__max_features\":[500,1000,1500,2000],\n",
    "    \"cvec__min_df\": [2,3],\n",
    "    \"cvec__max_df\": [0.9,0.95],\n",
    "    \"cvec__ngram_range\": [(1,1),(1,2)],\n",
    "    \"cvec__stop_words\": [None,\"english\"]\n",
    "}\n",
    "\n",
    "# ngram_range of (1,1) just returns individual tokens\n",
    "# ngram_range of (1,2) just returns individual tokens AND bi-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9f9cb62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate GridSearchCV.\n",
    "\n",
    "gs = GridSearchCV(pipe, # what object are we optimizing?\n",
    "                  param_grid=pipe_params, # what parameters values are we searching?\n",
    "                  cv=5) # 5-fold cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1177e6a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit GridSearch to training data.\n",
    "start_time = time.time()\n",
    "gs.fit(X_train,y_train)\n",
    "print(f\"Runtime:{time.time()-start_time}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ad36f42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# What's the best score?\n",
    "gs.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd67dfaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# What's the best params?\n",
    "gs.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f747520d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Score model on training set.\n",
    "# What is the score on a classifier? Accuracy\n",
    "gs.score(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e0d0d29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Score model on testing set.\n",
    "gs.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e41f58f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define score model dataframe for comparison\n",
    "list_of_rows = []\n",
    "score_df = pd.DataFrame()\n",
    "\n",
    "def modelscore(gsmodel,transformer,model,cheatword):\n",
    "    s = pd.Series({\"Transformer\": transformer, #\"CountVectorizer\",\"Tfid\" \n",
    "               \"Model\" : model, #\"MultinomialNB\",\"Logistic\"\n",
    "               \"Cheat Word\" : cheatword , #\"Included\" , \"Excluded\"\n",
    "               \"Train Score\" : gsmodel.score(X_train,y_train),\n",
    "               \"Test Score\" : gsmodel.score(X_test,y_test)\n",
    "               })\n",
    "    list_of_rows.append(s) \n",
    "    global score_df\n",
    "    score_df = pd.DataFrame(list_of_rows)\n",
    "    print(score_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f593ba07",
   "metadata": {},
   "outputs": [],
   "source": [
    "modelscore(gs, \"CountVectorizer\", \"MultinomialNB\", \"Excluded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d9177de",
   "metadata": {},
   "outputs": [],
   "source": [
    "score_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4896d050",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's set a pipeline up with two stages:\n",
    "# 1. TfidfVectorizer (transformer)\n",
    "# 2. Multinomial Naive Bayes (estimator)\n",
    "\n",
    "pipe_tvec = Pipeline([\n",
    "    (\"tvec\", TfidfVectorizer()), # Transformer (fit, transform)\n",
    "    (\"nb\", MultinomialNB()) # Estimator or model (fit, predict)    \n",
    "])\n",
    "\n",
    "# .predict() of MultinomialNB allows us to have a score to judge\n",
    "# our hyperparameters combinations when GridSearching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a822bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search over the following values of hyperparameters:\n",
    "# Maximum number of features fit: 500, 1000, 1500, 2000\n",
    "# No stop words and english stop words\n",
    "# Check (individual tokens) and also check (individual tokens and 2-grams).\n",
    "\n",
    "pipe_tvec_params = {\"tvec__max_features\" : [500,1000,1500,2000],\n",
    "                    \"tvec__min_df\": [2,3],\n",
    "                    \"tvec__max_df\": [0.9,0.95],\n",
    "                    \"tvec__stop_words\" : [None, \"english\"],\n",
    "                    \"tvec__ngram_range\" : [(1,1),(1,2)]\n",
    "                  }\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2009f748",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate GridSearchCV.\n",
    "gs_tvec = GridSearchCV(estimator=pipe_tvec,\n",
    "                      param_grid=pipe_tvec_params,\n",
    "                      cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10acedfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit GridSearch to training data.\n",
    "gs_tvec.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3471c1a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_tvec.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b72c6e2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Score model on training set.\n",
    "gs_tvec.score(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55e8583a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Score model on testing set.\n",
    "gs_tvec.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf83a9f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "modelscore(gs_tvec, \"TfidfVectorizer\", \"MultinomialNB\", \"Excluded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce9cd684",
   "metadata": {},
   "source": [
    "### Find top occuring words after remove game name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c612ab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the transformer.\n",
    "tvec = TfidfVectorizer(stop_words=\"english\")\n",
    "\n",
    "# convert training data to dataframe\n",
    "X_train_df = pd.DataFrame(tvec.fit_transform(X_train).todense(), \n",
    "                          columns=tvec.get_feature_names_out())\n",
    "# Plot top 20 occuring words\n",
    "X_train_df.sum().sort_values(ascending=True).tail(20).plot(kind='barh')\n",
    "plt.title(\"Top 20 Common Words after removing game name\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "153c082e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb6c9f47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's set a pipeline up with two stages:\n",
    "# 1. CountfVectorizer (transformer)\n",
    "# 2. LogisticRegression (estimator)\n",
    "\n",
    "pipe_cvec_log = Pipeline([\n",
    "    (\"cvec\", CountVectorizer()), # Transformer (fit, transform)\n",
    "    (\"logreg\", LogisticRegression()) # Estimator or model (fit, predict)    \n",
    "])\n",
    "\n",
    "# .predict() of MultinomialNB allows us to have a score to judge\n",
    "# our hyperparameters combinations when GridSearching\n",
    "\n",
    "# Search over the following values of hyperparameters:\n",
    "# Maximum number of features fit: 500, 1000, 1500, 2000\n",
    "# No stop words and english stop words\n",
    "# Check (individual tokens) and also check (individual tokens and 2-grams).\n",
    "\n",
    "pipe_cvec_log_params = {\"cvec__max_features\":[500, 1000, 1500, 2000],\n",
    "                        \"cvec__min_df\": [2,3],\n",
    "                        \"cvec__max_df\": [0.9,0.95],\n",
    "                        \"cvec__ngram_range\": [(1,1),(1,2)],\n",
    "                        \"cvec__stop_words\": [None,\"english\"],\n",
    "                       }\n",
    "    \n",
    "\n",
    "# Instantiate GridSearchCV.\n",
    "gs_cvec_log = GridSearchCV(estimator=pipe_cvec_log,\n",
    "                      param_grid=pipe_cvec_log_params,\n",
    "                      cv=5)\n",
    "\n",
    "# Fit GridSearch to training data.\n",
    "gs_cvec_log.fit(X_train,y_train)\n",
    "\n",
    "# Check best parameter\n",
    "print(gs_cvec_log.best_params_)\n",
    "\n",
    "# Score model on training set.\n",
    "gs_cvec_log.score(X_train,y_train)\n",
    "\n",
    "# Score model on testing set.\n",
    "gs_cvec_log.score(X_test,y_test)\n",
    "\n",
    "# Prediction\n",
    "y_preds_cvec_log = gs_cvec_log.predict(X_test)\n",
    "\n",
    "# Save model score to score dataframe\n",
    "modelscore(gs_cvec_log, \"CountVectorizer\", \"Logistic\", \"Excluded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c46d2c7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "313bad76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's set a pipeline up with two stages:\n",
    "# 1. TfidVectorizer (transformer)\n",
    "# 2. LogisticRegression (estimator)\n",
    "\n",
    "pipe_tvec_log = Pipeline([\n",
    "    (\"tvec\", TfidfVectorizer()), # Transformer (fit, transform)\n",
    "    (\"logreg\", LogisticRegression()) # Estimator or model (fit, predict)    \n",
    "])\n",
    "\n",
    "# .predict() of MultinomialNB allows us to have a score to judge\n",
    "# our hyperparameters combinations when GridSearching\n",
    "\n",
    "# Search over the following values of hyperparameters:\n",
    "# Maximum number of features fit: 2000, 3000, 4000, 5000\n",
    "# No stop words and english stop words\n",
    "# Check (individual tokens) and also check (individual tokens and 2-grams).\n",
    "\n",
    "pipe_tvec_log_params = {\"tvec__max_features\" : [2000,3000,4000,5000],\n",
    "                        \"tvec__min_df\": [2,3],\n",
    "                        \"tvec__max_df\": [0.9,0.95],\n",
    "                        \"tvec__stop_words\" : [None, \"english\"],\n",
    "                        \"tvec__ngram_range\" : [(1,1),(1,2)]\n",
    "                       }\n",
    "    \n",
    "\n",
    "# Instantiate GridSearchCV.\n",
    "gs_tvec_log = GridSearchCV(estimator=pipe_tvec_log,\n",
    "                      param_grid=pipe_tvec_log_params,\n",
    "                      cv=5)\n",
    "\n",
    "# Fit GridSearch to training data.\n",
    "gs_tvec_log.fit(X_train,y_train)\n",
    "\n",
    "gs_tvec_log.best_params_\n",
    "\n",
    "# Score model on training set.\n",
    "gs_tvec_log.score(X_train,y_train)\n",
    "\n",
    "# Score model on testing set.\n",
    "gs_tvec_log.score(X_test,y_test)\n",
    "\n",
    "# Prediction\n",
    "y_preds_tvec_log = gs_tvec_log.predict(X_test)\n",
    "\n",
    "# Save model score to score dataframe\n",
    "modelscore(gs_tvec_log, \"TfidfVectorizer\", \"Logistic\", \"Excluded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "655dd418",
   "metadata": {},
   "outputs": [],
   "source": [
    "score_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33ec555d",
   "metadata": {},
   "source": [
    "**Error Analysis**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7954e11b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8c36477",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate Model\n",
    "cvec = CountVectorizer(stop_words = \"english\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2dd47e1",
   "metadata": {},
   "source": [
    "'cvec__max_df': 0.9, 'cvec__max_features': 5000, 'cvec__min_df': 2, 'cvec__ngram_range': (1, 2), 'cvec__stop_words': None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b9ee2f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit\n",
    "cvec.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "339124b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform the corpus\n",
    "X_train = cvec.transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f7581cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.DataFrame(X_train.todense(), \n",
    "                        columns=cvec.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91ab948c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df[\"target\"] = y_train.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3fb948c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.groupby('target').sum()\\\n",
    ".T.sort_values(1, ascending=True).tail(20).plot.barh();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf042ea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_test, y_preds_tvec_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "470214c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "tn, fp, fn, tp = confusion_matrix(y_test, y_preds_tvec_log).ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef82b958",
   "metadata": {},
   "outputs": [],
   "source": [
    "ConfusionMatrixDisplay(cm, display_labels=gs_tvec_log.classes_).plot();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eda30e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "fp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdb41c34",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.DataFrame(X_test)\n",
    "test_df['actual_class'] = y_test\n",
    "test_df['predict_class'] = y_preds_tvec_log\n",
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cb79c28",
   "metadata": {},
   "outputs": [],
   "source": [
    "false_df = test_df[test_df['actual_class'] != test_df['predict_class']]['title_clean']\n",
    "false_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50427b53",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9410d840",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sample(30)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
